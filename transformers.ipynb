{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformer (From Scratch)"
      ],
      "metadata": {
        "id": "kA3TIudLetTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nJ7pBJwCL-NT"
      },
      "outputs": [],
      "source": [
        "# gere les structures de donnees de bases, le calcul du gradient automatique\n",
        "import torch\n",
        "# contient les blocs de construction des RN, couches pre-construites\n",
        "import torch.nn as nn\n",
        "# contient les algos d'optimisation\n",
        "import torch.optim as optim\n",
        "# pour creer les dataloaders (gere les flux de donnees)\n",
        "import torch.utils.data as data\n",
        "# contient les fonctions mathematiques\n",
        "import math\n",
        "# pour copier les onjets\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FWYvELFbL_L1"
      },
      "outputs": [],
      "source": [
        "class AttentionMultiTetes(nn.Module):\n",
        "    def __init__(self, d_modele, nb_tetes):\n",
        "        super(AttentionMultiTetes, self).__init__()\n",
        "        # S'assurer que la dimension du modèle (d_modele) est divisible par le nombre de têtes\n",
        "        assert d_modele % nb_tetes == 0, \"d_modele doit être divisible par nb_tetes\"\n",
        "\n",
        "        # Initialisation des dimensions\n",
        "        self.d_modele = d_modele # Dimension du modèle\n",
        "        self.nb_tetes = nb_tetes # Nombre de têtes d'attention\n",
        "        self.d_k = d_modele // nb_tetes # Dimension des clés, requêtes et valeurs de chaque tête\n",
        "\n",
        "        # Couches linéaires pour la transformation des entrées\n",
        "        self.W_q = nn.Linear(d_modele, d_modele) # Transformation des Requêtes (Query)\n",
        "        self.W_k = nn.Linear(d_modele, d_modele) # Transformation des Clés (Key)\n",
        "        self.W_v = nn.Linear(d_modele, d_modele) # Transformation des Valeurs (Value)\n",
        "        self.W_o = nn.Linear(d_modele, d_modele) # Transformation de Sortie (Output)\n",
        "\n",
        "    def attention_produit_scalaire_normalisee(self, Q, K, V, masque=None):\n",
        "        # Calcul des scores d'attention\n",
        "        scores_attn = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Appliquer le masque si fourni (utile pour ignorer le rembourrage/padding)\n",
        "        if masque is not None:\n",
        "            scores_attn = scores_attn.masked_fill(masque == 0, -1e9)\n",
        "\n",
        "        # Application du Softmax pour obtenir les probabilités d'attention\n",
        "        probs_attn = torch.softmax(scores_attn, dim=-1)\n",
        "\n",
        "        # Multiplier par les valeurs pour obtenir la sortie finale\n",
        "        sortie = torch.matmul(probs_attn, V)\n",
        "        return sortie\n",
        "\n",
        "    def separer_tetes(self, x):\n",
        "        # Redimensionner l'entrée pour avoir plusieurs têtes d'attention\n",
        "        taille_batch, long_seq, d_modele = x.size()\n",
        "        return x.view(taille_batch, long_seq, self.nb_tetes, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combiner_tetes(self, x):\n",
        "        # Combiner les têtes multiples pour revenir à la forme originale\n",
        "        taille_batch, _, long_seq, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(taille_batch, long_seq, self.d_modele)\n",
        "\n",
        "    def forward(self, Q, K, V, masque=None):\n",
        "        # Appliquer les transformations linéaires et séparer les têtes\n",
        "        Q = self.separer_tetes(self.W_q(Q))\n",
        "        K = self.separer_tetes(self.W_k(K))\n",
        "        V = self.separer_tetes(self.W_v(V))\n",
        "\n",
        "        # Effectuer l'attention par produit scalaire normalisée\n",
        "        sortie_attn = self.attention_produit_scalaire_normalisee(Q, K, V, masque)\n",
        "\n",
        "        # Combiner les têtes et appliquer la transformation de sortie finale\n",
        "        sortie = self.W_o(self.combiner_tetes(sortie_attn))\n",
        "        return sortie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCQbnUgCL_WD"
      },
      "outputs": [],
      "source": [
        "class ReseauNeuronesPositionnel(nn.Module):\n",
        "    def __init__(self, d_modele, d_ff):\n",
        "        super(ReseauNeuronesPositionnel, self).__init__()\n",
        "        # Première couche linéaire (expansion de la dimension)\n",
        "        self.fc1 = nn.Linear(d_modele, d_ff)\n",
        "        # Seconde couche linéaire (retour à la dimension originale du modèle)\n",
        "        self.fc2 = nn.Linear(d_ff, d_modele)\n",
        "        # Fonction d'activation non-linéaire\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage dans la première couche, activation ReLU, puis seconde couche\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RRBpV50EMHZy"
      },
      "outputs": [],
      "source": [
        "class EncodagePositionnel(nn.Module):\n",
        "    def __init__(self, d_modele, long_max_seq):\n",
        "        super(EncodagePositionnel, self).__init__()\n",
        "\n",
        "        # Création d'une matrice d'encodage positionnel (pe) remplie de zéros\n",
        "        pe = torch.zeros(long_max_seq, d_modele)\n",
        "\n",
        "        # Création d'un vecteur de positions (0, 1, 2, ..., long_max_seq)\n",
        "        position = torch.arange(0, long_max_seq, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Calcul du terme de division pour les fréquences sinus et cosinus\n",
        "        terme_div = torch.exp(torch.arange(0, d_modele, 2).float() * -(math.log(10000.0) / d_modele))\n",
        "\n",
        "        # Application de la fonction sinus aux indices pairs (0, 2, 4...)\n",
        "        pe[:, 0::2] = torch.sin(position * terme_div)\n",
        "\n",
        "        # Application de la fonction cosinus aux indices impairs (1, 3, 5...)\n",
        "        pe[:, 1::2] = torch.cos(position * terme_div)\n",
        "\n",
        "        # Enregistrer 'pe' comme un buffer (ne sera pas considéré comme un paramètre entraînable)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ajoute l'encodage positionnel aux plongements (embeddings) d'entrée\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lEpwimGRMHqg"
      },
      "outputs": [],
      "source": [
        "class CoucheEncodeur(nn.Module):\n",
        "    def __init__(self, d_modele, nb_tetes, d_ff, dropout):\n",
        "        super(CoucheEncodeur, self).__init__()\n",
        "        # Auto-attention multi-têtes\n",
        "        self.auto_attn = AttentionMultiTetes(d_modele, nb_tetes) ### ici\n",
        "        # Réseau de neurones positionnel (Feed-Forward)\n",
        "        self.reseau_positionnel = ReseauNeuronesPositionnel(d_modele, d_ff)\n",
        "        # Couches de normalisation\n",
        "        self.norm1 = nn.LayerNorm(d_modele)\n",
        "        self.norm2 = nn.LayerNorm(d_modele)\n",
        "        # Couche de dropout pour la régularisation\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, masque):\n",
        "        # Étape 1 : Auto-attention et connexion résiduelle suivie d'une normalisation\n",
        "        sortie_attn = self.auto_attn(x, x, x, masque)\n",
        "        x = self.norm1(x + self.dropout(sortie_attn))\n",
        "\n",
        "        # Étape 2 : Réseau Feed-Forward et connexion résiduelle suivie d'une normalisation\n",
        "        sortie_ff = self.reseau_positionnel(x)\n",
        "        x = self.norm2(x + self.dropout(sortie_ff))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6tV42gv-McxH"
      },
      "outputs": [],
      "source": [
        "class CoucheDecodeur(nn.Module):\n",
        "    def __init__(self, d_modele, nb_tetes, d_ff, dropout):\n",
        "        super(CoucheDecodeur, self).__init__()\n",
        "        # Auto-attention pour les tokens de la cible (déjà générés)\n",
        "        self.auto_attn = AttentionMultiTetes(d_modele, nb_tetes)\n",
        "        # Attention croisée pour regarder la sortie de l'encodeur\n",
        "        self.attn_croisee = AttentionMultiTetes(d_modele, nb_tetes)\n",
        "        # Réseau de neurones positionnel (Feed-Forward)\n",
        "        self.reseau_positionnel = ReseauNeuronesPositionnel(d_modele, d_ff)\n",
        "\n",
        "        # Couches de normalisation\n",
        "        self.norm1 = nn.LayerNorm(d_modele)\n",
        "        self.norm2 = nn.LayerNorm(d_modele)\n",
        "        self.norm3 = nn.LayerNorm(d_modele)\n",
        "        # Couche de dropout pour la régularisation\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sortie_encodeur, masque_src, masque_tgt):\n",
        "        # Étape 1 : Auto-attention sur la cible avec masque (pour ne pas voir le futur)\n",
        "        sortie_auto_attn = self.auto_attn(x, x, x, masque_tgt)\n",
        "        x = self.norm1(x + self.dropout(sortie_auto_attn))\n",
        "\n",
        "        # Étape 2 : Attention croisée (Requête vient du décodeur, Clé/Valeur de l'encodeur)\n",
        "        sortie_attn_croisee = self.attn_croisee(x, sortie_encodeur, sortie_encodeur, masque_src)\n",
        "        x = self.norm2(x + self.dropout(sortie_attn_croisee))\n",
        "\n",
        "        # Étape 3 : Réseau Feed-Forward\n",
        "        sortie_ff = self.reseau_positionnel(x)\n",
        "        x = self.norm3(x + self.dropout(sortie_ff))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cZ8_ylccMc37"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, taille_vocab_src, taille_vocab_tgt, d_modele, nb_tetes, nb_couches, d_ff, long_max_seq, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Couches de plongement (embeddings) pour la source et la cible\n",
        "        self.plongement_encodeur = nn.Embedding(taille_vocab_src, d_modele)\n",
        "        self.plongement_decodeur = nn.Embedding(taille_vocab_tgt, d_modele)\n",
        "        # Module d'encodage positionnel\n",
        "        self.encodage_positionnel = EncodagePositionnel(d_modele, long_max_seq)\n",
        "\n",
        "        # Listes de couches pour l'encodeur et le décodeur\n",
        "        self.couches_encodeur = nn.ModuleList([CoucheEncodeur(d_modele, nb_tetes, d_ff, dropout) for _ in range(nb_couches)])\n",
        "        self.couches_decodeur = nn.ModuleList([CoucheDecodeur(d_modele, nb_tetes, d_ff, dropout) for _ in range(nb_couches)])\n",
        "\n",
        "        # Couche linéaire finale pour la prédiction des mots\n",
        "        self.fc = nn.Linear(d_modele, taille_vocab_tgt)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generer_masque(self, src, tgt):\n",
        "        # Masque pour ignorer les jetons de rembourrage (padding) dans la source\n",
        "        masque_src = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        # Masque pour ignorer le rembourrage dans la cible\n",
        "        masque_tgt = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "        # Masque triangulaire pour empêcher le décodeur de regarder les mots futurs\n",
        "        long_seq = tgt.size(1)\n",
        "        masque_causal = (1 - torch.triu(torch.ones(1, long_seq, long_seq), diagonal=1)).bool()\n",
        "        masque_tgt = masque_tgt & masque_causal\n",
        "\n",
        "        return masque_src, masque_tgt\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # 1. Génération des masques\n",
        "        masque_src, masque_tgt = self.generer_masque(src, tgt)\n",
        "\n",
        "        # 2. Préparation des entrées (Embedding + Encodage Positionnel)\n",
        "        src_embedded = self.dropout(self.encodage_positionnel(self.plongement_encodeur(src)))\n",
        "        tgt_embedded = self.dropout(self.encodage_positionnel(self.plongement_decodeur(tgt)))\n",
        "\n",
        "        # 3. Passage à travers les couches de l'encodeur\n",
        "        sortie_encodeur = src_embedded\n",
        "        for couche_enc in self.couches_encodeur:\n",
        "            sortie_encodeur = couche_enc(sortie_encodeur, masque_src)\n",
        "\n",
        "        # 4. Passage à travers les couches du décodeur\n",
        "        sortie_decodeur = tgt_embedded\n",
        "        for couche_dec in self.couches_decodeur:\n",
        "            sortie_decodeur = couche_dec(sortie_decodeur, sortie_encodeur, masque_src, masque_tgt)\n",
        "\n",
        "        # 5. Projection finale vers le vocabulaire de sortie\n",
        "        output = self.fc(sortie_decodeur)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1C0D5NuFMmBX"
      },
      "outputs": [],
      "source": [
        "# Configuration des hyperparamètres\n",
        "taille_vocab_src = 5000\n",
        "taille_vocab_tgt = 5000\n",
        "d_modele = 512\n",
        "nb_tetes = 8\n",
        "nb_couches = 6\n",
        "d_ff = 2048\n",
        "long_max_seq = 5000  # Augmenté pour tester les limites du modèle\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialisation du modèle avec les noms de variables traduits\n",
        "transformer = Transformer(\n",
        "    taille_vocab_src,\n",
        "    taille_vocab_tgt,\n",
        "    d_modele,\n",
        "    nb_tetes,\n",
        "    nb_couches,\n",
        "    d_ff,\n",
        "    long_max_seq,\n",
        "    dropout\n",
        ")\n",
        "\n",
        "# Génération de données d'exemple aléatoires\n",
        "# src_donnees : (taille_batch, long_sequence)\n",
        "taille_batch = 64\n",
        "src_donnees = torch.randint(1, taille_vocab_src, (taille_batch, long_max_seq))\n",
        "tgt_donnees = torch.randint(1, taille_vocab_tgt, (taille_batch, long_max_seq))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atzsfGk4WXxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def benchmark_transformer_performances(modele, device=\"cuda\"):\n",
        "    tailles_sequence = [128, 256, 512, 1024, 1536, 2048]\n",
        "    temps_execution = []\n",
        "    memoire_allouee = []\n",
        "\n",
        "    modele.to(device)\n",
        "    modele.eval()\n",
        "\n",
        "    print(f\"Début du benchmark sur {device}...\")\n",
        "\n",
        "    for taille in tailles_sequence:\n",
        "        # Création de données fictives (Batch de 1)\n",
        "        src = torch.randint(1, 5000, (1, taille)).to(device)\n",
        "        tgt = torch.randint(1, 5000, (1, taille)).to(device)\n",
        "\n",
        "        torch.cuda.synchronize() # Attendre que le GPU soit prêt\n",
        "        torch.cuda.empty_cache()\n",
        "        mem_initiale = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "\n",
        "        debut = time.time()\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                _ = modele(src, tgt[:, :-1])\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            fin = time.time()\n",
        "\n",
        "            mem_finale = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
        "\n",
        "            temps_execution.append(fin - debut)\n",
        "            memoire_allouee.append(mem_finale - mem_initiale)\n",
        "            print(f\"Taille {taille} : {fin-debut:.4f}s | {mem_finale:.2f} MB\")\n",
        "\n",
        "        except RuntimeError: # En cas de Out of Memory (OOM)\n",
        "            print(f\"Taille {taille} : ÉCHEC (Mémoire insuffisante)\")\n",
        "            temps_execution.append(None)\n",
        "            memoire_allouee.append(None)\n",
        "            break\n",
        "\n",
        "    return tailles_sequence, temps_execution, memoire_allouee\n",
        "\n",
        "# Exécution\n",
        "tailles, temps, memoire = benchmark_transformer_performances(transformer, device=\"gpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "dFatuz31WX9G",
        "outputId": "32e18b81-5a3f-4ac5-bb9c-b7fa3f370bb3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2447248357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Exécution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtailles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_transformer_performances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2447248357.py\u001b[0m in \u001b[0;36mbenchmark_transformer_performances\u001b[0;34m(modele, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmemoire_allouee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \"\"\"\n\u001b[0;32m-> 1330\u001b[0;31m         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tracer_graphiques(tailles, temps, memoire):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Courbe du temps\n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Longueur de la séquence (n)')\n",
        "    ax1.set_ylabel('Temps d\\'exécution (s)', color=color)\n",
        "    ax1.plot(tailles[:len(temps)], temps, marker='o', color=color, label=\"Temps (Quadratique)\")\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    # Courbe de la mémoire\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Mémoire GPU (MB)', color=color)\n",
        "    ax2.plot(tailles[:len(memoire)], memoire, marker='s', color=color, label=\"Mémoire (Quadratique)\")\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    plt.title(\"L'impact de la complexité quadratique n²\")\n",
        "    plt.grid(True, linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "tracer_graphiques(tailles, temps, memoire)"
      ],
      "metadata": {
        "id": "0_ngLGdgWY0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Xb1OPeCWaKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhtTWYk4MmKM"
      },
      "outputs": [],
      "source": [
        "# Définition de la fonction de perte (entropie croisée)\n",
        "# On ignore l'index 0 qui correspond généralement au rembourrage (padding)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Configuration de l'optimiseur Adam avec les paramètres du papier original\n",
        "optimiser = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Passage du modèle en mode entraînement\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    # Réinitialisation des gradients (indispensable à chaque étape)\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Passage en avant (Forward pass) : on donne la source et la cible (décalée d'un mot)\n",
        "    # tgt_donnees[:, :-1] signifie qu'on donne tout sauf le dernier mot\n",
        "    sortie = transformer(src_donnees, tgt_donnees[:, :-1])\n",
        "\n",
        "    # Calcul de la perte : on compare la prédiction avec la cible attendue (décalée à droite)\n",
        "    # view(-1) permet d'aplatir les tenseurs pour le calcul de l'entropie croisée\n",
        "    perte = criterion(sortie.contiguous().view(-1, taille_vocab_tgt),\n",
        "                   tgt_donnees[:, 1:].contiguous().view(-1))\n",
        "\n",
        "    # Rétropropagation de l'erreur (Calcul des gradients)\n",
        "    perte.backward()\n",
        "\n",
        "    # Mise à jour des poids du Transformer\n",
        "    optimiser.step()\n",
        "\n",
        "    # Affichage de la perte pour suivre la progression\n",
        "    print(f\"Époque: {epoch+1}, Perte: {perte.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ3Mh3TtM5_8"
      },
      "outputs": [],
      "source": [
        "# Passage du modèle en mode évaluation (désactive le dropout)\n",
        "transformer.eval()\n",
        "\n",
        "# Génération de données de validation aléatoires\n",
        "# val_src_donnees : données sources de validation\n",
        "# val_tgt_donnees : données cibles de validation\n",
        "taille_batch = 64\n",
        "val_src_donnees = torch.randint(1, taille_vocab_src, (taille_batch, long_max_seq))\n",
        "val_tgt_donnees = torch.randint(1, taille_vocab_tgt, (taille_batch, long_max_seq))\n",
        "\n",
        "# Désactivation du calcul des gradients (économie de mémoire et de calcul)\n",
        "with torch.no_grad():\n",
        "    # Passage en avant sur les données de validation\n",
        "    val_sortie = transformer(val_src_donnees, val_tgt_donnees[:, :-1])\n",
        "\n",
        "    # Calcul de la perte de validation\n",
        "    perte_val = criterion(val_sortie.contiguous().view(-1, taille_vocab_tgt),\n",
        "                        val_tgt_donnees[:, 1:].contiguous().view(-1))\n",
        "\n",
        "    print(f\"Perte de Validation : {perte_val.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lCDAwD8XOxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser un tokeniser pré-entraîné (celui de GPT-2 ou BERT) pour gagner du temps sur la gestion du vocabulaire"
      ],
      "metadata": {
        "id": "2ZYWjIapbXGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Utilisation du tokeniser de GPT-2\n",
        "nom_tokeniser = \"gpt2\"\n",
        "tokeniser = AutoTokenizer.from_pretrained(nom_tokeniser)\n",
        "\n",
        "# Ajout d'un jeton de rembourrage (padding) car GPT-2 n'en a pas par défaut\n",
        "tokeniser.pad_token = tokeniser.eos_token"
      ],
      "metadata": {
        "id": "_e7QX4OrXO7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# environ 100 mots\n",
        "document_court = \"\"\"\n",
        "L'intelligence artificielle transforme radicalement notre façon de travailler.\n",
        "Les modèles de langage, comme les Transformers, permettent aujourd'hui de\n",
        "résumer des textes, de traduire des langues et de générer du code.\n",
        "Cependant, ces modèles font face à un défi majeur : la gestion des documents\n",
        "très longs à cause de leur complexité mathématique quadratique.\n",
        "\"\"\"\n",
        "\n",
        "# Encodage du texte\n",
        "entrees = tokeniser(document_court, return_tensors=\"pt\", padding=True, truncation=True, max_length=long_max_seq)\n",
        "src_doc = entrees['input_ids'] # Notre source pour le Transformer"
      ],
      "metadata": {
        "id": "lia2vsiWbdty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On simule une cible (le début d'un résumé par exemple)\n",
        "cible_fictive = torch.randint(1, taille_vocab_tgt, (1, 10))\n",
        "\n",
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    # Passage du vrai document dans le modèle\n",
        "    prediction = transformer(src_doc, cible_fictive)\n",
        "\n",
        "print(f\"Forme de l'entrée : {src_doc.shape}\") # (1, longueur_du_texte)\n",
        "print(f\"Forme de la sortie : {prediction.shape}\")"
      ],
      "metadata": {
        "id": "hJThHMaubksm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_long = \"cava comee cai joe amdje oa\""
      ],
      "metadata": {
        "id": "d_D3ZdM5bpej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. On s'assure que le vocabulaire correspond au tokeniser\n",
        "taille_vocab = len(tokeniser) # Généralement 50257 pour GPT-2\n",
        "\n",
        "# 2. On réinitialise le modèle correctement\n",
        "transformer = Transformer(\n",
        "    taille_vocab_src=taille_vocab,\n",
        "    taille_vocab_tgt=taille_vocab,\n",
        "    d_modele=512,\n",
        "    nb_tetes=8,\n",
        "    nb_couches=6,\n",
        "    d_ff=2048,\n",
        "    long_max_seq=10000, # Doit être > à ton document\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# 3. On encode ton document long\n",
        "# Imaginons que 'document_long' est ta variable texte\n",
        "entrees = tokeniser(document_long, return_tensors=\"pt\", truncation=True, max_length=10000)\n",
        "src_doc = entrees['input_ids']\n",
        "\n",
        "# 4. Pour le test de complexité, on simule une cible de résumé de 100 mots\n",
        "cible_resume = torch.randint(1, taille_vocab, (1, 100))\n",
        "\n",
        "# 5. Appel\n",
        "prediction = transformer(src_doc, cible_resume)"
      ],
      "metadata": {
        "id": "3kRfAqvnc1nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-5FTvtcdEJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}